# -*- coding: utf-8 -*-
"""AI3_23013

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zGnUMwiI7mBaujfyNih0Hlky73J0lrrh
"""

'''!pip install ucimlrepo
!pip install pgmpy'''

import subprocess
packages_to_install = ['ucimlrepo', 'numpy','sklearn','pgmpy','networkx']
for package in packages_to_install:
  subprocess.check_call(['pip', 'install', package])
"""**Installed necessary packages**

Importing the packages needed to be used
"""

import matplotlib.pyplot as plt
import seaborn as sns
from pgmpy.inference import VariableElimination
import pandas as pd
from ucimlrepo import fetch_ucirepo
import networkx as nx
import numpy as np
from sklearn.preprocessing import KBinsDiscretizer
from pgmpy.models import BayesianModel
from pgmpy.estimators import TreeSearch
from pgmpy.estimators import ParameterEstimator, MaximumLikelihoodEstimator
from pgmpy.models import BayesianNetwork
from sklearn.metrics import accuracy_score
import random
from pgmpy.estimators import BDeuScore
from pgmpy.estimators import HillClimbSearch
from sklearn.feature_selection import mutual_info_classif
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

"""Getting the wine dataset and storing the same"""

wine = fetch_ucirepo(id=109)
X = wine.data.features
y = wine.data.targets
data=pd.read_csv('https://archive.ics.uci.edu/static/public/109/data.csv')

data.shape

"""Custom functions for doing the discretization"""

def estimate_bins(data):
    n = len(data)
    if n == 0:
        return 0
    bin_width = 3.5 * np.std(data) / np.power(n, 1/3)
    num_bins = int(np.ceil((max(data) - min(data)) / bin_width))
    return num_bins

def discretize(data,num_bins):

  discretizer = KBinsDiscretizer(n_bins=num_bins, encode='ordinal', strategy='uniform')
  X_discretized = discretizer.fit_transform(X)
  X_discretized_df = pd.DataFrame(X_discretized, columns=X.columns)
  return X_discretized

column_values=data.columns[:-1]

for i in column_values:
  new_col=discretize(data[i],estimate_bins(data[i]))
  data[i]=new_col

# continuous columns to be discretized
continuous_columns = ['Alcohol', 'Flavanoids', 'Malicacid','Alcalinity_of_ash','Ash','Total_phenols','Nonflavanoid_phenols','Proanthocyanins']

# Creating a copy of the DataFrame to avoid modifying the original
discretized_data = data.copy()

# Apply discretization to selected columns
for column in continuous_columns:
    # Define the number of bins and the discretization strategy
    n_bins = 5
    strategy = 'kmeans'

    # Apply KBinsDiscretizer
    discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)
    discretized_values = discretizer.fit_transform(data[[column]])
    discretized_data[column] = discretized_values.flatten()

"""Doing tree search on the discretized data"""

data = discretized_data.copy()
tree_search = TreeSearch(data,root_node='class')
best_model = tree_search.estimate()
print("Edges in new Bayesian network after tree search :", best_model.edges())

print(len(best_model))
print(best_model.edges())

"""Creating the bayesian network after finding the proper structure and fitting in with the data"""

A= BayesianNetwork(best_model.edges())
A.fit(discretized_data, estimator=MaximumLikelihoodEstimator)

"""Printing the probability distribution"""

# A is our Bayesian model
inference = VariableElimination(A)

# dropping class column from 'discretized_data'
discretized_data_without_class = discretized_data.drop(columns=['class'])

# Create a separate bar plot for each row
for index, row in discretized_data_without_class.iterrows():
    # Convert the current row to a dictionary
    evidence_dict = row.to_dict()
    # Inference to get the probability distribution
    probability_distribution = inference.query(variables=['class'], evidence=evidence_dict)

    # Extract probabilities and class values
    values_of_class = probability_distribution.state_names['class']
    probabilities = probability_distribution.values.flatten()  # Flatten the array
    plt.figure(figsize=(8, 6))
    sns.barplot(x=values_of_class, y=probabilities, palette="viridis")
    plt.title(f"Probability Distribution for Row {index}")
    plt.xlabel("Class")
    plt.ylabel("Probability")
    plt.show()

edges = A.edges()

# Create a directed graph
graph = nx.DiGraph()

# Add edges to the graph
graph.add_edges_from(edges)

# Plot the graph
pos = nx.spring_layout(graph)
nx.draw(graph, pos, with_labels=True, font_weight='bold', arrowsize=20, node_size=700, node_color='green', font_color='black')

# Show the plot
plt.show()

"""The bayesian network that we got finally"""

print(len(best_model))

"""Here, we perform inference and find the accuracy by finding the prediction values"""

inference = VariableElimination(A)

# Perform inference to predict the 'class' variable for each row in the dataset
predicted_labels = []
for _, row in discretized_data.iterrows():
    evidence = row.drop('class').to_dict()
    result = inference.map_query(variables=['class'], evidence=evidence)
    predicted_labels.append(result['class'])

# Actual class labels from the original data
actual_labels = discretized_data['class']

# Calculate accuracy
accuracy_A = accuracy_score(actual_labels, predicted_labels)

inference = VariableElimination(A)

pruned_model = A.copy()
for edge in A.edges():
    source, target = edge

    # Check if the source and target nodes are present in the model
    if source in pruned_model.nodes() and target in pruned_model.nodes():
        pruned_model[source][target]['weight'] = 0.2 if random.choice([True, False]) else 0.8

# Print edges after pruning
print("Edges in the pruned Bayesian network:", pruned_model.edges())

"""Model after pruning my custom logic"""

B = pruned_model.copy()
# Creating a copy to avoid modifying the original model
data = discretized_data.copy()
# Initialize the BDeu score
bdeu = BDeuScore(data)

initial_score = bdeu.score(B)

# Identify edges to be pruned
edges_to_prune = []

# Iterate over edges and identify those that do not significantly affect the score
for edge in B.edges():
    # Temporarily remove the edge and calculate the new score
    temp_model = B.copy()
    temp_model.remove_edge(*edge)
    new_score = bdeu.score(temp_model)
    # Compare the scores and add the edge to the list if the decrease is not significant
    if new_score > initial_score:
        edges_to_prune.append(edge)

# Remove the identified edges
for edge in edges_to_prune:
    B.remove_edge(*edge)

# Print the pruned edges
print("Pruned Edges:", B.edges())

node_to_prune = 'Alcalinity_of_ash'

# Get all edges connected to the node
edges_to_remove = list(B.in_edges(node_to_prune)) + list(B.out_edges(node_to_prune))

# Remove the identified edges
for edge in edges_to_remove:
    B.remove_edge(*edge)

# Remove the node itself
B.remove_node(node_to_prune)

# Print the pruned edges
print("Pruned Edges:", B.edges())

# Assuming 'best_model' is your Bayesian network and 'discretized_data' is your DataFrame
inference = VariableElimination(B)

# Perform inference to predict the 'class' variable for each row in the dataset
predicted_labels = []
for _, row in discretized_data.drop(columns=['Alcalinity_of_ash']).iterrows():
    evidence = row.drop('class').to_dict()
    result = inference.map_query(variables=['class'], evidence=evidence)
    predicted_labels.append(result['class'])

# Actual class labels from the original data
actual_labels = discretized_data['class']
accuracy_B = accuracy_score(actual_labels, predicted_labels)

data = discretized_data.copy()

# Use HillClimbSearch with BDeuScore and minimum indegree constraint
hill_climb = HillClimbSearch(data)
# Conduct the search with the minimum indegree constraint
best_model_C = hill_climb.estimate(max_indegree=2,scoring_method='k2score')

"""Accuracy for the model C"""

C= BayesianNetwork(best_model_C.edges())
C.fit(discretized_data, estimator=MaximumLikelihoodEstimator)

inference = VariableElimination(C)
# Perform inference to predict the 'class' variable for each row in the dataset
predicted_labels = []
for _, row in discretized_data.iterrows():
    evidence = row.drop('class').to_dict()
    result = inference.map_query(variables=['class'], evidence=evidence)
    predicted_labels.append(result['class'])

# Actual class labels from the original data
actual_labels = discretized_data['class']

# Calculate accuracy
accuracy_C = accuracy_score(actual_labels, predicted_labels)

X = discretized_data.drop(columns=['class'])
y = discretized_data['class']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest Classifier to get feature importance
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)
feature_importance = clf.feature_importances_
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})

feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Select the top two features
selected_features = feature_importance_df.head(2)['Feature'].tolist()
selected_data = discretized_data[selected_features + ['class']]

C= BayesianNetwork(best_model.edges())
C.fit(discretized_data, estimator=MaximumLikelihoodEstimator)

# C is the Bayesian model
inference = VariableElimination(C)

evidence_data = discretized_data[['Hue', 'Proline']]

# Convert evidence_data to a dictionary
evidence_dict = evidence_data.iloc[142].to_dict()

# Perform inference to get the probability distribution
probability_distribution = inference.query(variables=['class'], evidence=evidence_dict)

# Extract probabilities and class values
values_of_class = probability_distribution.state_names['class']
probabilities = probability_distribution.values.flatten()  # Flatten the array
plt.figure(figsize=(8, 6))
sns.barplot(x=values_of_class, y=probabilities, palette="viridis")
plt.title(f"P(class | Hue={evidence_dict['Hue']}, Proline={evidence_dict['Proline']})")
plt.xlabel("Class")
plt.ylabel("Probability")
plt.show()

"""Plotted the probability distribution"""

'''Compute for at least 4 cases the posterior probabilities of variable(s)
of interest (queries) given evidence (observations).'''

for i in range(5):
  probability_distribution = inference.query(variables=['class'], evidence=evidence_data.iloc[4+i*9].to_dict())
  print(probability_distribution )

"""Yes, the the results match your
intuition and/or domain knowledge becasue in the dataset the same values are given so they have to correct
"""

print("final accuracies are")
print(accuracy_A)
print(accuracy_B)
print(accuracy_C)